@article{gregor2015draw,
  title={DRAW: A recurrent neural network for image generation},
  author={Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
  journal={arXiv preprint, arXiv:1502.04623},
  year={2015},
  url={https://arxiv.org/pdf/1502.04623.pdf}
}


@article{li2023starcoder,
      title={StarCoder: may the source be with you!}, 
      author={Raymond Li and Loubna Ben Allal and Yangtian Zi and Niklas Muennighoff and Denis Kocetkov and Chenghao Mou and Marc Marone and Christopher Akiki and Jia Li and Jenny Chim and Qian Liu and Evgenii Zheltonozhskii and Terry Yue Zhuo and Thomas Wang and Olivier Dehaene and Mishig Davaadorj and Joel Lamy-Poirier and João Monteiro and Oleh Shliazhko and Nicolas Gontier and Nicholas Meade and Armel Zebaze and Ming-Ho Yee and Logesh Kumar Umapathi and Jian Zhu and Benjamin Lipkin and Muhtasham Oblokulov and Zhiruo Wang and Rudra Murthy and Jason Stillerman and Siva Sankalp Patel and Dmitry Abulkhanov and Marco Zocca and Manan Dey and Zhihan Zhang and Nour Fahmy and Urvashi Bhattacharyya and Wenhao Yu and Swayam Singh and Sasha Luccioni and Paulo Villegas and Maxim Kunakov and Fedor Zhdanov and Manuel Romero and Tony Lee and Nadav Timor and Jennifer Ding and Claire Schlesinger and Hailey Schoelkopf and Jan Ebert and Tri Dao and Mayank Mishra and Alex Gu and Jennifer Robinson and Carolyn Jane Anderson and Brendan Dolan-Gavitt and Danish Contractor and Siva Reddy and Daniel Fried and Dzmitry Bahdanau and Yacine Jernite and Carlos Muñoz Ferrandis and Sean Hughes and Thomas Wolf and Arjun Guha and Leandro von Werra and Harm de Vries},
      year={2023},
      eprint={2305.06161},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{
  penedo2024the,
  title={The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale},
  author={Guilherme Penedo and Hynek Kydl{\'\i}{\v{c}}ek and Loubna Ben allal and Anton Lozhkov and Margaret Mitchell and Colin Raffel and Leandro Von Werra and Thomas Wolf},
  booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2024},
  url={https://openreview.net/forum?id=n6SCkn2QaG}
}


@article{wang2024taming,
  title={Taming Sensitive Weights: Noise Perturbation Fine-tuning for Robust LLM Quantization},
  author={Wang, Dongwei and Yang, Huanrui},
  journal={arXiv preprint arXiv:2412.06858},
  year={2024}
}

# large activations 
@article{sun2024massive,
  title={Massive activations in large language models},
  author={Sun, Mingjie and Chen, Xinlei and Kolter, J Zico and Liu, Zhuang},
  journal={arXiv preprint arXiv:2402.17762},
  year={2024}
}

@article{xuimproved,
  title={Improved Fully Quantized Training via Rectifying Batch Normalization},
  author={Xu, Kaixin and Lin, Jie and Wang, Zhe and Hu, Peng and Zhao, Ziyuan},
  journal={arXiv preprint arXiv:},
  year={2023}
}

@article{shi2024continual,
  title={Continual learning of large language models: A comprehensive survey},
  author={Shi, Haizhou and Xu, Zihao and Wang, Hengyi and Qin, Weiyi and Wang, Wenyuan and Wang, Yibin and Wang, Zifeng and Ebrahimi, Sayna and Wang, Hao},
  journal={arXiv preprint arXiv:2404.16789},
  year={2024}
}

@article{gupta2023continual,
  title={Continual pre-training of large language models: How to (re) warm your model?},
  author={Gupta, Kshitij and Th{\'e}rien, Benjamin and Ibrahim, Adam and Richter, Mats L and Anthony, Quentin and Belilovsky, Eugene and Rish, Irina and Lesort, Timoth{\'e}e},
  journal={arXiv preprint arXiv:2308.04014},
  year={2023}
}

@article{merrill2022saturated,
  title={Saturated transformers are constant-depth threshold circuits},
  author={Merrill, William and Sabharwal, Ashish and Smith, Noah A},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={843--856},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{petty2023impact,
  title={The impact of depth and width on transformer language model generalization},
  author={Petty, Jackson and van Steenkiste, Sjoerd and Sha, Fei and Dasgupta, Ishita and Garrette, Dan and Linzen, Tal},
  year={2023}
}

# 
@article{fishman2024scaling,
  title={Scaling FP8 training to trillion-token LLMs},
  author={Fishman, Maxim and Chmiel, Brian and Banner, Ron and Soudry, Daniel},
  journal={arXiv preprint arXiv:2409.12517},
  year={2024}
}

@misc{nanogpt,
  author = {Andrej Karpathy},
  title = {The simplest, fastest repository for training/finetuning medium-sized GPTs.},
  year = {2022},
  url = {https://github.com/karpathy/nanoGPT},
  urldate = {2025-04-05}
}

@misc{llama4,
  author = {AI Meta},
  title = {The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation},
  year = {2025},
  url = {https://ai.meta.com/blog/llama-4-multimodal-intelligence/},
  urldate = {2025-04-05}
}

@article{grattafiori2024llama,
  title={The llama 3 herd of models},
  author={Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{wang2025optimizing,
  title={Optimizing Large Language Model Training Using FP4 Quantization},
  author={Wang, Ruizhe and Gong, Yeyun and Liu, Xiao and Zhao, Guoshuai and Yang, Ziyue and Guo, Baining and Zha, Zhengjun and Cheng, Peng},
  journal={arXiv preprint arXiv:2501.17116},
  year={2025}
}





#
@article{zhao2023pytorch,
  title={Pytorch fsdp: experiences on scaling fully sharded data parallel},
  author={Zhao, Yanli and Gu, Andrew and Varma, Rohan and Luo, Liang and Huang, Chien-Chin and Xu, Min and Wright, Less and Shojanazeri, Hamid and Ott, Myle and Shleifer, Sam and others},
  journal={arXiv preprint arXiv:2304.11277},
  year={2023}
}

# 
@article{groeneveld2024olmo,
  title={Olmo: Accelerating the science of language models},
  author={Groeneveld, Dirk and Beltagy, Iz and Walsh, Pete and Bhagia, Akshita and Kinney, Rodney and Tafjord, Oyvind and Jha, Ananya Harsh and Ivison, Hamish and Magnusson, Ian and Wang, Yizhong and others},
  journal={arXiv preprint arXiv:2402.00838},
  year={2024}
}

@article{muennighoff2024olmoe,
  title={Olmoe: Open mixture-of-experts language models},
  author={Muennighoff, Niklas and Soldaini, Luca and Groeneveld, Dirk and Lo, Kyle and Morrison, Jacob and Min, Sewon and Shi, Weijia and Walsh, Pete and Tafjord, Oyvind and Lambert, Nathan and others},
  journal={arXiv preprint arXiv:2409.02060},
  year={2024}
}

# Nvidia
@misc{supportmx,
  author = {NVIDIA},
  title = {OpenAI Triton on NVIDIA Blackwell Boosts AI Performance and Programmability},
  year = {2025},
  url = {https://git},
  urldate = {2025-02-05}
}

@misc{TransformerEngine,
  author = {NVIDIA},
  title = {},
  year = {2024},
  url = {https://github.com/NVIDIA/TransformerEngine},
  urldate = {2025-0-18}
}

@misc{website1,
  author = {NVIDIA},
  title = {NVIDIA Blackwell Platform Arrives to Power a New Era of Computing},
  year = {2024},
  url = {https://nvidianews.nvidia.com/news/nvidia-blackwell-platform-arrives-to-power-a-new-era-of-computing},
  urldate = {2025-03-18}
}

@misc{website3,
  author = {NVIDIA},
  title = {NVIDIA Blackwell Architecture},
  year = {2025},
  url = {https://resources.nvidia.com/en-us-blackwell-architecture},
  urldate = {2025-04-02}
}


# quantization
@article{liu2024deepseek,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@article{rouhani2023microscaling,
  title={Microscaling data formats for deep learning},
  author={Rouhani, Bita Darvish and Zhao, Ritchie and More, Ankit and Hall, Mathew and Khodamoradi, Alireza and Deng, Summer and Choudhary, Dhruv and Cornea, Marius and Dellinger, Eric and Denolf, Kristof and others},
  journal={arXiv preprint arXiv:2310.10537},
  year={2023}
}

@inproceedings{darvish2023shared,
  title={With shared microexponents, a little shifting goes a long way},
  author={Darvish Rouhani, Bita and Zhao, Ritchie and Elango, Venmugil and Shafipour, Rasoul and Hall, Mathew and Mesmakhosroshahi, Maral and More, Ankit and Melnick, Levi and Golub, Maximilian and Varatkar, Girish and others},
  booktitle={Proceedings of the 50th Annual International Symposium on Computer Architecture},
  pages={1--13},
  year={2023}
}

@misc{ocp_mx,
  author = {Darvish Rouhani, Bita and Garegrat, Nitin and  Savell, Tom and More, Ankit and Han, Kyung-Nam and Zhao, Ritchie  and Hall, Mathew },
  title = {Open Compute Project},
  year = {2023},
  url = {https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf},
  urldate = {2025-04-05}
}

@misc{mx_library,
  author = {{Microsoft}},
  title = {MX Pytorch Emulation Library},
  year = {2024},
  url = {https://github.com/microsoft/microxcaling/tree/main},
  urldate = {2025-05-15}
}


# Scaling Law
@article{kumar2024scaling,
  title={Scaling laws for precision},
  author={Kumar, Tanishq and Ankner, Zachary and Spector, Benjamin F and Bordelon, Blake and Muennighoff, Niklas and Paul, Mansheej and Pehlevan, Cengiz and R{\'e}, Christopher and Raghunathan, Aditi},
  journal={arXiv preprint arXiv:2411.04330},
  year={2024}
}

@article{liu2025paretoq,
  title={ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization},
  author={Liu, Zechun and Zhao, Changsheng and Huang, Hanxian and Chen, Sijia and Zhang, Jing and Zhao, Jiawei and Roy, Scott and Jin, Lisa and Xiong, Yunyang and Shi, Yangyang and others},
  journal={arXiv preprint arXiv:2502.02631},
  year={2025}
}


@article{ouyang2024low,
  title={Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens},
  author={Ouyang, Xu and Ge, Tao and Hartvigsen, Thomas and Zhang, Zhisong and Mi, Haitao and Yu, Dong},
  journal={arXiv preprint arXiv:2411.17691},
  year={2024}
}

@misc{cohere2025commandaenterprisereadylarge,
      title={Command A: An Enterprise-Ready Large Language Model}, 
      author={Team Cohere and : and Aakanksha and Arash Ahmadian and Marwan Ahmed and Jay Alammar and Milad Alizadeh and Yazeed Alnumay and Sophia Althammer and Arkady Arkhangorodsky and Viraat Aryabumi and Dennis Aumiller and Raphaël Avalos and Zahara Aviv and Sammie Bae and Saurabh Baji and Alexandre Barbet and Max Bartolo and Björn Bebensee and Neeral Beladia and Walter Beller-Morales and Alexandre Bérard and Andrew Berneshawi and Anna Bialas and Phil Blunsom and Matt Bobkin and Adi Bongale and Sam Braun and Maxime Brunet and Samuel Cahyawijaya and David Cairuz and Jon Ander Campos and Cassie Cao and Kris Cao and Roman Castagné and Julián Cendrero and Leila Chan Currie and Yash Chandak and Diane Chang and Giannis Chatziveroglou and Hongyu Chen and Claire Cheng and Alexis Chevalier and Justin T. Chiu and Eugene Cho and Eugene Choi and Eujeong Choi and Tim Chung and Volkan Cirik and Ana Cismaru and Pierre Clavier and Henry Conklin and Lucas Crawhall-Stein and Devon Crouse and Andres Felipe Cruz-Salinas and Ben Cyrus and Daniel D'souza and Hugo Dalla-Torre and John Dang and William Darling and Omar Darwiche Domingues and Saurabh Dash and Antoine Debugne and Théo Dehaze and Shaan Desai and Joan Devassy and Rishit Dholakia and Kyle Duffy and Ali Edalati and Ace Eldeib and Abdullah Elkady and Sarah Elsharkawy and Irem Ergün and Beyza Ermis and Marzieh Fadaee and Boyu Fan and Lucas Fayoux and Yannis Flet-Berliac and Nick Frosst and Matthias Gallé and Wojciech Galuba and Utsav Garg and Matthieu Geist and Mohammad Gheshlaghi Azar and Ellen Gilsenan-McMahon and Seraphina Goldfarb-Tarrant and Tomas Goldsack and Aidan Gomez and Victor Machado Gonzaga and Nithya Govindarajan and Manoj Govindassamy and Nathan Grinsztajn and Nikolas Gritsch and Patrick Gu and Shangmin Guo and Kilian Haefeli and Rod Hajjar and Tim Hawes and Jingyi He and Sebastian Hofstätter and Sungjin Hong and Sara Hooker and Tom Hosking and Stephanie Howe and Eric Hu and Renjie Huang and Hemant Jain and Ritika Jain and Nick Jakobi and Madeline Jenkins and JJ Jordan and Dhruti Joshi and Jason Jung and Trushant Kalyanpur and Siddhartha Rao Kamalakara and Julia Kedrzycki and Gokce Keskin and Edward Kim and Joon Kim and Wei-Yin Ko and Tom Kocmi and Michael Kozakov and Wojciech Kryściński and Arnav Kumar Jain and Komal Kumar Teru and Sander Land and Michael Lasby and Olivia Lasche and Justin Lee and Patrick Lewis and Jeffrey Li and Jonathan Li and Hangyu Lin and Acyr Locatelli and Kevin Luong and Raymond Ma and Lukáš Mach and Marina Machado and Joanne Magbitang and Brenda Malacara Lopez and Aryan Mann and Kelly Marchisio and Olivia Markham and Alexandre Matton and Alex McKinney and Dominic McLoughlin and Jozef Mokry and Adrien Morisot and Autumn Moulder and Harry Moynehan and Maximilian Mozes and Vivek Muppalla and Lidiya Murakhovska and Hemangani Nagarajan and Alekhya Nandula and Hisham Nasir and Shauna Nehra and Josh Netto-Rosen and Daniel Ohashi and James Owers-Bardsley and Jason Ozuzu and Dennis Padilla and Gloria Park and Sam Passaglia and Jeremy Pekmez and Laura Penstone and Aleksandra Piktus and Case Ploeg and Andrew Poulton and Youran Qi and Shubha Raghvendra and Miguel Ramos and Ekagra Ranjan and Pierre Richemond and Cécile Robert-Michon and Aurélien Rodriguez and Sudip Roy and Sebastian Ruder and Laura Ruis and Louise Rust and Anubhav Sachan and Alejandro Salamanca and Kailash Karthik Saravanakumar and Isha Satyakam and Alice Schoenauer Sebag and Priyanka Sen and Sholeh Sepehri and Preethi Seshadri and Ye Shen and Tom Sherborne and Sylvie Shang Shi and Sanal Shivaprasad and Vladyslav Shmyhlo and Anirudh Shrinivason and Inna Shteinbuk and Amir Shukayev and Mathieu Simard and Ella Snyder and Ava Spataru and Victoria Spooner and Trisha Starostina and Florian Strub and Yixuan Su and Jimin Sun and Dwarak Talupuru and Eugene Tarassov and Elena Tommasone and Jennifer Tracey and Billy Trend and Evren Tumer and Ahmet Üstün and Bharat Venkitesh and David Venuto and Pat Verga and Maxime Voisin and Alex Wang and Donglu Wang and Shijian Wang and Edmond Wen and Naomi White and Jesse Willman and Marysia Winkels and Chen Xia and Jessica Xie and Minjie Xu and Bowen Yang and Tan Yi-Chern and Ivan Zhang and Zhenyu Zhao and Zhoujie Zhao},
      year={2025},
      eprint={2504.00698},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.00698}, 
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@inproceedings{dettmers2023case,
  title={The case for 4-bit precision: k-bit inference scaling laws},
  author={Dettmers, Tim and Zettlemoyer, Luke},
  booktitle={International Conference on Machine Learning},
  pages={7750--7774},
  year={2023},
  organization={PMLR}
}

@article{brandfonbrener2024loss,
      title={Loss-to-Loss Prediction: Scaling Laws for All Datasets}, 
      author={Brandfonbrener, David and Anand, Nikhil and Vyas, Nikhil and Malach, Eran and Kakade, Sham},
      journal={arXiv preprint arXiv:2411.12925},
      year={2024}
}

## stochastic rounding
@article{duchi2012randomized,
  title={Randomized smoothing for stochastic optimization},
  author={Duchi, John C and Bartlett, Peter L and Wainwright, Martin J},
  journal={SIAM Journal on Optimization},
  volume={22},
  number={2},
  pages={674--701},
  year={2012},
  publisher={SIAM}
}

@article{zhao2024direct,
  title={Direct Quantized Training of Language Models with Stochastic Rounding},
  author={Zhao, Kaiyan and Tabaru, Tsuguchika and Kobayashi, Kenichi and Honda, Takumi and Yamazaki, Masafumi and Tsuruoka, Yoshimasa},
  journal={arXiv preprint arXiv:2412.04787},
  year={2024}
}

@inproceedings{gupta2015deep,
  title={Deep learning with limited numerical precision},
  author={Gupta, Suyog and Agrawal, Ankur and Gopalakrishnan, Kailash and Narayanan, Pritish},
  booktitle={International conference on machine learning},
  pages={1737--1746},
  year={2015},
  organization={PMLR}
}

@article{wang2018training,
  title={Training deep neural networks with 8-bit floating point numbers},
  author={Wang, Naigang and Choi, Jungwook and Brand, Daniel and Chen, Chia-Yu and Gopalakrishnan, Kailash},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{ali2024stochastic,
  title={A Stochastic Rounding-Enabled Low-Precision Floating-Point MAC for DNN Training},
  author={Ali, Sami Ben and Filip, Silviu-Ioan and Sentieys, Olivier},
  booktitle={2024 Design, Automation \& Test in Europe Conference \& Exhibition (DATE)},
  pages={1--6},
  year={2024},
  organization={IEEE}
}

@article{von1947numerical,
  title={Numerical inverting of matrices of high order},
  author={Von Neumann, John and Goldstine, Herman Heine},
  year={1947}
}

## FP8 
@article{cohere2025command,
  title={Command A: An Enterprise-Ready Large Language Model},
  author={Cohere, Team and Ahmadian, Arash and Ahmed, Marwan and Alammar, Jay and Alnumay, Yazeed and Althammer, Sophia and Arkhangorodsky, Arkady and Aryabumi, Viraat and Aumiller, Dennis and Avalos, Rapha{\"e}l and others},
  journal={arXiv preprint arXiv:2504.00698},
  year={2025}
}

@article{micikevicius2022fp8,
  title={Fp8 formats for deep learning},
  author={Micikevicius, Paulius and Stosic, Dusan and Burgess, Neil and Cornea, Marius and Dubey, Pradeep and Grisenthwaite, Richard and Ha, Sangwon and Heinecke, Alexander and Judd, Patrick and Kamalu, John and others},
  journal={arXiv preprint arXiv:2209.05433},
  year={2022}
}





## Sharding
@article{wang2024domino,
  title={Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping},
  author={Wang, Guanhua and Zhang, Chengming and Shen, Zheyu and Li, Ang and Ruwase, Olatunji},
  journal={arXiv preprint arXiv:2409.15241},
  year={2024}
}


@misc{dao2022flashattentionfastmemoryefficientexact,
      title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness}, 
      author={Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and Christopher Ré},
      year={2022},
      eprint={2205.14135},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2205.14135}, 
}


@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@misc{sahoo2024simpleeffectivemaskeddiffusion,
      title={Simple and Effective Masked Diffusion Language Models}, 
      author={Subham Sekhar Sahoo and Marianne Arriola and Yair Schiff and Aaron Gokaslan and Edgar Marroquin and Justin T Chiu and Alexander Rush and Volodymyr Kuleshov},
      year={2024},
      eprint={2406.07524},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.07524}, 
}

@misc{raffel2023exploringlimitstransferlearning,
      title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, 
      author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
      year={2023},
      eprint={1910.10683},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.10683}, 
}

@misc{lee2025fp8againquantifyingreduced,
      title={To FP8 and Back Again: Quantifying Reduced Precision Effects on LLM Training Stability}, 
      author={Joonhyung Lee and Jeongin Bae and Byeongwook Kim and Se Jung Kwon and Dongsoo Lee},
      year={2025},
      eprint={2405.18710},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.18710}, 
}

@misc{jastrzebski2020breakevenpointoptimizationtrajectories,
      title={The Break-Even Point on Optimization Trajectories of Deep Neural Networks}, 
      author={Stanislaw Jastrzebski and Maciej Szymczak and Stanislav Fort and Devansh Arpit and Jacek Tabor and Kyunghyun Cho and Krzysztof Geras},
      year={2020},
      eprint={2002.09572},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.09572}, 
}
@misc{micikevicius2022fp8formatsdeeplearning,
      title={FP8 Formats for Deep Learning}, 
      author={Paulius Micikevicius and Dusan Stosic and Neil Burgess and Marius Cornea and Pradeep Dubey and Richard Grisenthwaite and Sangwon Ha and Alexander Heinecke and Patrick Judd and John Kamalu and Naveen Mellempudi and Stuart Oberman and Mohammad Shoeybi and Michael Siu and Hao Wu},
      year={2022},
      eprint={2209.05433},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2209.05433}, 
}

@inproceedings{DBLP:conf/iclr/WortsmanLXEAACG24,
  author       = {Mitchell Wortsman and
                  Peter J. Liu and
                  Lechao Xiao and
                  Katie E. Everett and
                  Alexander A. Alemi and
                  Ben Adlam and
                  John D. Co{-}Reyes and
                  Izzeddin Gur and
                  Abhishek Kumar and
                  Roman Novak and
                  Jeffrey Pennington and
                  Jascha Sohl{-}Dickstein and
                  Kelvin Xu and
                  Jaehoon Lee and
                  Justin Gilmer and
                  Simon Kornblith},
  title        = {Small-scale proxies for large-scale Transformer training instabilities},
  booktitle    = {The Twelfth International Conference on Learning Representations,
                  {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=d8w0pmvXbZ},
  timestamp    = {Mon, 29 Jul 2024 17:17:48 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/WortsmanLXEAACG24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{shazeer2020gluvariantsimprovetransformer,
      title={GLU Variants Improve Transformer}, 
      author={Noam Shazeer},
      year={2020},
      eprint={2002.05202},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.05202}, 
}

@misc{tseng2025trainingllmsmxfp4,
      title={Training LLMs with MXFP4}, 
      author={Albert Tseng and Tao Yu and Youngsuk Park},
      year={2025},
      eprint={2502.20586},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.20586}, 
}

@misc{porian2025resolvingdiscrepanciescomputeoptimalscaling,
      title={Resolving Discrepancies in Compute-Optimal Scaling of Language Models}, 
      author={Tomer Porian and Mitchell Wortsman and Jenia Jitsev and Ludwig Schmidt and Yair Carmon},
      year={2025},
      eprint={2406.19146},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.19146}, 
}

@article{
ibrahim2024simple,
title={Simple and Scalable Strategies to Continually Pre-train Large Language Models},
author={Adam Ibrahim and Benjamin Th{\'e}rien and Kshitij Gupta and Mats Leon Richter and Quentin Gregory Anthony and Eugene Belilovsky and Timoth{\'e}e Lesort and Irina Rish},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=DimPeeCxKO},
note={}
}

@misc{bondarenko2023quantizabletransformersremovingoutliers,
      title={Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing}, 
      author={Yelysei Bondarenko and Markus Nagel and Tijmen Blankevoort},
      year={2023},
      eprint={2306.12929},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2306.12929}, 
}

@misc{molybog2023theoryadaminstabilitylargescale,
      title={A Theory on Adam Instability in Large-Scale Machine Learning}, 
      author={Igor Molybog and Peter Albert and Moya Chen and Zachary DeVito and David Esiobu and Naman Goyal and Punit Singh Koura and Sharan Narang and Andrew Poulton and Ruan Silva and Binh Tang and Diana Liskovich and Puxin Xu and Yuchen Zhang and Melanie Kambadur and Stephen Roller and Susan Zhang},
      year={2023},
      eprint={2304.09871},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2304.09871}, 
}

@misc{zhang2022optopenpretrainedtransformer,
      title={OPT: Open Pre-trained Transformer Language Models}, 
      author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
      year={2022},
      eprint={2205.01068},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2205.01068}, 
}

@misc{dehghani2023scalingvisiontransformers22,
      title={Scaling Vision Transformers to 22 Billion Parameters}, 
      author={Mostafa Dehghani and Josip Djolonga and Basil Mustafa and Piotr Padlewski and Jonathan Heek and Justin Gilmer and Andreas Steiner and Mathilde Caron and Robert Geirhos and Ibrahim Alabdulmohsin and Rodolphe Jenatton and Lucas Beyer and Michael Tschannen and Anurag Arnab and Xiao Wang and Carlos Riquelme and Matthias Minderer and Joan Puigcerver and Utku Evci and Manoj Kumar and Sjoerd van Steenkiste and Gamaleldin F. Elsayed and Aravindh Mahendran and Fisher Yu and Avital Oliver and Fantine Huot and Jasmijn Bastings and Mark Patrick Collier and Alexey Gritsenko and Vighnesh Birodkar and Cristina Vasconcelos and Yi Tay and Thomas Mensink and Alexander Kolesnikov and Filip Pavetić and Dustin Tran and Thomas Kipf and Mario Lučić and Xiaohua Zhai and Daniel Keysers and Jeremiah Harmsen and Neil Houlsby},
      year={2023},
      eprint={2302.05442},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2302.05442}, 
}

@misc{chowdhery2022palmscalinglanguagemodeling,
      title={PaLM: Scaling Language Modeling with Pathways}, 
      author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
      year={2022},
      eprint={2204.02311},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2204.02311}, 
}

@misc{zoph2022stmoedesigningstabletransferable,
      title={ST-MoE: Designing Stable and Transferable Sparse Expert Models}, 
      author={Barret Zoph and Irwan Bello and Sameer Kumar and Nan Du and Yanping Huang and Jeff Dean and Noam Shazeer and William Fedus},
      year={2022},
      eprint={2202.08906},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2202.08906}, 
}

@inproceedings{damianselfstabilization,
  author       = {Alex Damian and
                  Eshaan Nichani and
                  Jason D. Lee},
  title        = {Self-Stabilization: The Implicit Bias of Gradient Descent at the Edge
                  of Stability},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  publisher    = {OpenReview.net},
  year         = {2023},
  url          = {https://openreview.net/forum?id=nhKHA59gXz},
  timestamp    = {Wed, 24 Jul 2024 16:50:33 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/DamianNL23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/iclr/CohenKLKT21,
  author       = {Jeremy Cohen and
                  Simran Kaur and
                  Yuanzhi Li and
                  J. Zico Kolter and
                  Ameet Talwalkar},
  title        = {Gradient Descent on Neural Networks Typically Occurs at the Edge of
                  Stability},
  booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021,
                  Virtual Event, Austria, May 3-7, 2021},
  publisher    = {OpenReview.net},
  year         = {2021},
  url          = {https://openreview.net/forum?id=jh-rTtvkGeM},
  timestamp    = {Mon, 07 Aug 2023 17:37:03 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/CohenKLKT21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{querykey_ln,
  author       = {Alex Henry and
                  Prudhvi Raj Dachapally and
                  Shubham Shantaram Pawar and
                  Yuxuan Chen},
  title        = {Query-Key Normalization for Transformers},
  journal      = {CoRR},
  volume       = {abs/2010.04245},
  year         = {2020},
  url          = {https://arxiv.org/abs/2010.04245},
  eprinttype    = {arXiv},
  eprint       = {2010.04245},
  timestamp    = {Wed, 11 Nov 2020 16:20:26 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2010-04245.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{kaplan_scaling,
  author       = {Jared Kaplan and
                  Sam McCandlish and
                  Tom Henighan and
                  Tom B. Brown and
                  Benjamin Chess and
                  Rewon Child and
                  Scott Gray and
                  Alec Radford and
                  Jeffrey Wu and
                  Dario Amodei},
  title        = {Scaling Laws for Neural Language Models},
  journal      = {CoRR},
  volume       = {abs/2001.08361},
  year         = {2020},
  url          = {https://arxiv.org/abs/2001.08361},
  eprinttype    = {arXiv},
  eprint       = {2001.08361},
  timestamp    = {Wed, 03 Jun 2020 10:55:13 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2001-08361.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{ma2025understandingsilentdatacorruption,
      title={Understanding Silent Data Corruption in LLM Training}, 
      author={Jeffrey Ma and Hengzhi Pei and Leonard Lausen and George Karypis},
      year={2025},
      eprint={2502.12340},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.12340}, 
}

@misc{openai2025gpt45,
  author = {OpenAI},
  title = {GPT-4.5 System Card},
  year = {2025},
  howpublished = {\url{https://cdn.openai.com/gpt-4-5-system-card-2272025.pdf}},
  note = {Accessed: 2025-06-20}
}

@misc{anthropic2025claude4,
  author = {Anthropic},
  title = {Claude 4 System Card},
  year = {2025},
  howpublished = {\url{https://www-cdn.anthropic.com/4263b940cabb546aa0e3283f35b686f4f3b2ff47.pdf}},
  note = {Accessed: 2025-06-20}
}

@misc{deepmind2025gemini25,
  author = {Google DeepMind},
  title = {Gemini 2.5 Technical Report},
  year = {2025},
  howpublished = {\url{https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf}},
  note = {Accessed: 2025-06-20}
}


@misc{nair2025matryoshkaquantization,
      title={Matryoshka Quantization}, 
      author={Pranav Nair and Puranjay Datta and Jeff Dean and Prateek Jain and Aditya Kusupati},
      year={2025},
      eprint={2502.06786},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.06786}, 
}

@misc{jacob2017quantizationtrainingneuralnetworks,
      title={Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference}, 
      author={Benoit Jacob and Skirmantas Kligys and Bo Chen and Menglong Zhu and Matthew Tang and Andrew Howard and Hartwig Adam and Dmitry Kalenichenko},
      year={2017},
      eprint={1712.05877},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1712.05877}, 
}

@inproceedings{Abdolrashidi_2021,
   title={Pareto-Optimal Quantized ResNet Is Mostly 4-bit},
   url={http://dx.doi.org/10.1109/CVPRW53098.2021.00345},
   DOI={10.1109/cvprw53098.2021.00345},
   booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
   publisher={IEEE},
   author={Abdolrashidi, AmirAli and Wang, Lisa and Agrawal, Shivani and Malmaud, Jonathan and Rybakov, Oleg and Leichner, Chas and Lew, Lukasz},
   year={2021},
   month=jun, pages={3085–3093} }

@misc{shao2024omniquantomnidirectionallycalibratedquantization,
      title={OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models}, 
      author={Wenqi Shao and Mengzhao Chen and Zhaoyang Zhang and Peng Xu and Lirui Zhao and Zhiqian Li and Kaipeng Zhang and Peng Gao and Yu Qiao and Ping Luo},
      year={2024},
      eprint={2308.13137},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2308.13137}, 
}

@misc{chen2025scalinglawquantizationawaretraining,
      title={Scaling Law for Quantization-Aware Training}, 
      author={Mengzhao Chen and Chaoyi Zhang and Jing Liu and Yutao Zeng and Zeyue Xue and Zhiheng Liu and Yunshui Li and Jin Ma and Jie Huang and Xun Zhou and Ping Luo},
      year={2025},
      eprint={2505.14302},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2505.14302}, 
}

@misc{takase2025spikemorestabilizingpretraining,
      title={Spike No More: Stabilizing the Pre-training of Large Language Models}, 
      author={Sho Takase and Shun Kiyono and Sosuke Kobayashi and Jun Suzuki},
      year={2025},
      eprint={2312.16903},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.16903}, 
}

@misc{mishra2025recipespretrainingllmsmxfp8,
      title={Recipes for Pre-training LLMs with MXFP8}, 
      author={Asit Mishra and Dusan Stosic and Simon Layton and Paulius Micikevicius},
      year={2025},
      eprint={2506.08027},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2506.08027}, 
}

@misc{zhou2025linearlayoutsrobustcode,
      title={Linear Layouts: Robust Code Generation of Efficient Tensor Computation Using $\mathbb{F}_2$}, 
      author={Keren Zhou and Mario Lezcano and Adam Goucher and Akhmed Rakhmati and Jeff Niu and Justin Lebar and Pawel Szczerbuk and Peter Bell and Phil Tillet and Thomas Raoux and Zahi Moudallal},
      year={2025},
      eprint={2505.23819},
      archivePrefix={arXiv},
      primaryClass={cs.PL},
      url={https://arxiv.org/abs/2505.23819}, 
}

@misc{noune20228bitnumericalformatsdeep,
      title={8-bit Numerical Formats for Deep Neural Networks}, 
      author={Badreddine Noune and Philip Jones and Daniel Justus and Dominic Masters and Carlo Luschi},
      year={2022},
      eprint={2206.02915},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2206.02915}, 
}

@misc{lin2025scalinglawslinearregression,
      title={Scaling Laws in Linear Regression: Compute, Parameters, and Data}, 
      author={Licong Lin and Jingfeng Wu and Sham M. Kakade and Peter L. Bartlett and Jason D. Lee},
      year={2025},
      eprint={2406.08466},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.08466}, 
}

@misc{yang2022tensorprogramsvtuning,
      title={Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer}, 
      author={Greg Yang and Edward J. Hu and Igor Babuschkin and Szymon Sidor and Xiaodong Liu and David Farhi and Nick Ryder and Jakub Pachocki and Weizhu Chen and Jianfeng Gao},
      year={2022},
      eprint={2203.03466},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2203.03466}, 
}